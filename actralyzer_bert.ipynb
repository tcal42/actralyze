{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "actralyzer_bert.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOfTa/nrLoEnep/YV2iS/6T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcal42/actralyze/blob/master/actralyzer_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucVPB8HSRzI7",
        "colab_type": "code",
        "outputId": "9edac6b8-ecf1-4e90-d2d8-038e93eb2266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QabWtWdiU_XC",
        "colab_type": "code",
        "outputId": "7533d9c4-e7bb-401c-e832-a878fa6cd71e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9K7ylxpVB5r",
        "colab_type": "code",
        "outputId": "e7e570c5-96f2-40ef-ffc9-f40ec90ae0e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.9 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGsj0nuTVEJj",
        "colab_type": "code",
        "outputId": "e3c6f676-2a6c-430a-fb2d-f8bc5518356c",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f06df09a-43d2-4ba1-857a-df101304f8cd\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f06df09a-43d2-4ba1-857a-df101304f8cd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train_comments.csv to train_comments.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16BDXx7mV1GD",
        "colab_type": "code",
        "outputId": "ceb6a92b-02f8-4a71-913e-0a821df0b8a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"train_comments.csv\")\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 792\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>segment</th>\n",
              "      <th>category</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>528</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>overall satisfaction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Services on weekends is “no service.” Saturday...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>overall satisfaction</td>\n",
              "      <td>positive</td>\n",
              "      <td>My 3 1/2 months have exceeded my expectations.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>AL</td>\n",
              "      <td>activities</td>\n",
              "      <td>negative</td>\n",
              "      <td>I think it would be nice to have a few more si...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>AL</td>\n",
              "      <td>overall satisfaction</td>\n",
              "      <td>positive</td>\n",
              "      <td>Everything has gone as planned so far.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>IL-BBV</td>\n",
              "      <td>parking</td>\n",
              "      <td>negative</td>\n",
              "      <td>More free parking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>overall satisfaction</td>\n",
              "      <td>positive</td>\n",
              "      <td>Wonderful, compatible, well run, lovely inside...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>MC</td>\n",
              "      <td>overall satisfaction</td>\n",
              "      <td>positive</td>\n",
              "      <td>Upon meeting many Jane Elke at any time, I've ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>MC</td>\n",
              "      <td>food quality</td>\n",
              "      <td>negative</td>\n",
              "      <td>Better food - higher temperature in home</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>IL-BBV</td>\n",
              "      <td>food service</td>\n",
              "      <td>negative</td>\n",
              "      <td>Suggest dining room supervisors check what rem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>652</th>\n",
              "      <td>IL-BBV</td>\n",
              "      <td>medical services</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A life with trained nurses available instead o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    segment  ...                                            comment\n",
              "528  IL-ETH  ...  Services on weekends is “no service.” Saturday...\n",
              "356  IL-HSD  ...    My 3 1/2 months have exceeded my expectations. \n",
              "168     AL   ...  I think it would be nice to have a few more si...\n",
              "135     AL   ...             Everything has gone as planned so far.\n",
              "403  IL-BBV  ...                                  More free parking\n",
              "389  IL-HSD  ...  Wonderful, compatible, well run, lovely inside...\n",
              "257      MC  ...  Upon meeting many Jane Elke at any time, I've ...\n",
              "237      MC  ...           Better food - higher temperature in home\n",
              "481  IL-BBV  ...  Suggest dining room supervisors check what rem...\n",
              "652  IL-BBV  ...  A life with trained nurses available instead o...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "309YKckgWY4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "texts = df.comment.values\n",
        "# labels = df.category.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EonxQ_LSacq0",
        "colab_type": "code",
        "outputId": "debf2439-ced6-432f-c612-4f8d8d59a92e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "categories = df['category'].tolist()\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(categories)\n",
        "list(le.classes_)\n",
        "\n",
        "df['category_int'] = le.fit_transform(df['category'])\n",
        "labels = df.category_int.values\n",
        "\n",
        "labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([18, 11, 17,  6,  1,  0,  4, 18, 17, 17,  3,  4,  0, 18, 12,  7, 18,\n",
              "        4,  9,  4, 11, 18, 17, 11,  8,  0,  1,  1, 17,  4, 18, 17,  1, 17,\n",
              "       17, 17,  6,  7, 18, 12, 18,  7, 17,  1, 18, 17, 16, 18, 13, 14,  3,\n",
              "       17,  0, 16,  7, 18,  0,  3, 19, 18, 10,  1, 17,  5, 14, 14, 13, 14,\n",
              "        0,  0,  1, 13,  1,  8,  0, 17,  8, 18, 13, 14,  3, 14, 17, 17, 17,\n",
              "       17,  0, 17, 14, 14, 13, 17, 16, 17, 14,  4,  1,  1,  0,  6, 17,  1,\n",
              "       14, 10, 14, 19, 14, 17,  5, 17,  8,  3,  7, 19,  6, 17,  6,  7, 17,\n",
              "        6,  6, 17,  6, 17, 17,  0, 17,  8, 16,  8, 19, 14, 14,  8, 12, 14,\n",
              "        4, 17, 14, 16, 14,  1, 17,  0,  1, 17,  8,  1,  0,  3, 17, 14, 17,\n",
              "        0, 17,  4,  0, 13,  1, 13, 13, 11, 17,  1,  4,  6,  8, 17,  0, 17,\n",
              "       14, 14,  4, 16,  0, 17, 10, 17, 17, 13,  2,  1,  0, 17, 14,  4, 14,\n",
              "        6, 17, 14,  4, 14,  0, 13, 14, 17, 11, 11, 17, 16,  0,  1, 11, 17,\n",
              "       14,  3, 17, 13, 17,  1, 14, 17,  1, 14,  8, 12, 10,  4,  1, 14, 14,\n",
              "       17, 13, 13,  1, 17,  4,  8,  8, 17,  4,  0, 17,  1,  5,  5, 17,  6,\n",
              "       17, 11,  8,  1, 17, 17,  4, 18,  8, 17,  1, 17, 13, 11,  9, 16, 17,\n",
              "        8, 17, 14,  1,  1,  3,  1, 13,  4,  0,  8, 14, 11,  9, 18, 17,  0,\n",
              "       13, 17, 17, 13,  6,  0, 16, 17,  4, 11, 13, 17, 18,  1, 17, 14,  4,\n",
              "        6,  1, 13,  1, 17, 14, 17,  0, 17, 18, 18, 14,  2, 13, 14, 14, 10,\n",
              "        6, 15,  2, 14, 10, 14,  1, 14, 14, 17, 14, 17, 14, 11, 14, 14, 10,\n",
              "       13, 14, 17, 17, 14, 10,  2, 18, 17, 19, 14,  6, 14, 12, 15, 14, 11,\n",
              "        4, 17, 12, 15, 14, 15,  4, 14, 14, 15, 14, 14, 14, 14,  0, 14, 14,\n",
              "       14, 11, 11, 16, 10, 12, 14, 15, 14, 12, 17, 14, 11, 14,  6,  6, 19,\n",
              "       17, 14, 14,  2, 14,  5, 14, 13, 17,  4, 14, 14, 18, 17, 17, 14, 15,\n",
              "        0,  7,  7, 15,  7,  7,  7,  6,  7,  6,  7, 15, 15,  0,  7,  8, 12,\n",
              "       10, 15, 13, 12, 11, 13, 10,  8, 13,  7,  8,  6,  8,  6, 18, 19,  7,\n",
              "       19,  6,  7, 13,  8,  8,  6,  8, 13,  6,  0,  0, 13,  3,  5, 13,  6,\n",
              "        7,  8, 13,  5, 12, 13,  8, 12, 15, 13,  6,  5, 13,  5,  7, 11, 15,\n",
              "       15,  0,  0,  7, 11,  0, 19, 11,  5, 12, 10,  7,  6, 15,  2, 15,  5,\n",
              "        1,  5,  7, 13,  8,  7, 15, 15,  7, 14, 10, 19,  8, 19,  6, 15,  0,\n",
              "        3, 19,  2,  2, 13,  4,  3,  2,  0,  3,  0,  2,  0,  5,  0,  7, 11,\n",
              "        0,  5,  4,  2, 15, 19,  2, 12,  2,  4, 13,  1, 16,  2, 14, 11,  2,\n",
              "       19, 14,  2, 19,  1, 11,  5,  0, 11,  0,  4, 16, 13,  4, 17, 16,  2,\n",
              "        2,  1,  0,  0, 17,  4, 13,  2,  5,  7, 15,  5, 19, 19, 13,  7,  3,\n",
              "        2,  3,  0,  3,  0, 10, 10,  3,  3,  2,  2, 19,  0,  4, 17,  2, 13,\n",
              "        7,  0,  7,  0, 12, 12,  2,  2, 11,  0,  2,  0,  3,  2, 12, 11,  5,\n",
              "       16, 10,  7,  7, 19, 12, 12, 12,  4, 12, 16,  1, 12, 10, 14,  2,  5,\n",
              "        7,  3, 13,  2,  3, 12, 10,  7, 12,  7,  7, 18, 13, 13, 12, 12,  2,\n",
              "       14,  7,  2,  7, 12, 16, 16,  5,  7,  2, 12,  2, 12, 19, 12,  0, 14,\n",
              "       10, 16,  2,  2, 10, 12, 12,  3,  1, 17, 11,  2, 11,  3,  7, 11, 13,\n",
              "       12,  7, 11, 12,  7, 13, 12, 13, 13, 11,  2, 13, 19, 11,  5,  6,  1,\n",
              "        7,  3, 12,  7, 14,  7, 16,  3,  0, 12,  7, 14, 11, 12, 17, 10, 13,\n",
              "        3,  3, 13, 17, 10, 16,  3, 10,  1,  8,  5,  3, 10, 12, 13,  8, 10,\n",
              "       16, 17,  2, 18, 16,  2, 13, 18, 17, 10, 13,  8, 13,  6,  8, 10,  7,\n",
              "       13,  2,  7, 17, 16, 11,  8, 17,  1, 11,  2,  8,  1,  7,  7, 15, 18,\n",
              "       18,  7, 13, 12,  8, 14, 16,  5, 10, 17, 17,  0,  2,  8,  0,  2,  6,\n",
              "       13, 16, 13, 10, 13,  1, 10,  8,  4,  3,  7, 12,  4, 19, 10, 16,  4,\n",
              "        5, 10, 12, 15,  0, 13, 16,  2,  3, 12])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKR0ycj0XqTD",
        "colab_type": "code",
        "outputId": "9f7e5c4a-2988-40c3-97f0-76ce9c6b630d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6RXURYBXs4i",
        "colab_type": "code",
        "outputId": "0ac70def-308b-41b6-f00f-2710ac0d7e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', texts[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(texts[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(texts[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  More CNA's\n",
            "Tokenized:  ['more', 'cn', '##a', \"'\", 's']\n",
            "Token IDs:  [2062, 27166, 2050, 1005, 1055]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPijNrcvX0xC",
        "colab_type": "code",
        "outputId": "819c78e0-2613-4b65-be02-dd07dc3bcfa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for text in texts:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_text = tokenizer.encode(\n",
        "                        text,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_text)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', texts[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1160 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  More CNA's\n",
            "Token IDs: [101, 2062, 27166, 2050, 1005, 1055, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP3ffYolYITX",
        "colab_type": "code",
        "outputId": "e99d4f50-9847-4516-cd9e-256c9450e66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print('Max sentence length: ', max([len(text) for text in input_ids]))\n",
        "print('Mean sentence length: ', np.mean([len(text) for text in input_ids]))\n",
        "print('Median sentence length: ', np.median([len(text) for text in input_ids]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  1162\n",
            "Mean sentence length:  38.89646464646464\n",
            "Median sentence length:  26.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgFw9t2mYbVp",
        "colab_type": "code",
        "outputId": "7c1c6807-9fe2-451a-dec7-7f9f8c88d37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64 \n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm6S65pSY_Uf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for text in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in text]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpfk4XhKZHtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHr0HqFHZOS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLNYZVmRZTHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wehAKEGlZhlQ",
        "colab_type": "code",
        "outputId": "03eb53a3-e48a-4ebd-84bf-27dea3b31b2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(le.classes_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xulE2HzubisH",
        "colab_type": "code",
        "outputId": "22904fe1-60d4-4224-e288-98ec63198e84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = len(le.classes_), # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bFqlqAMbxa0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHam0U09b-n8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 10\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0smYM1ipcA7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def flat_macro_f1_score(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return f1_score(labels_flat, pred_flat, average='macro')\n",
        "\n",
        "def flat_micro_f1_score(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return f1_score(labels_flat, pred_flat, average='micro')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_Tg0ffAcCzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul-t-T9ccGQO",
        "colab_type": "code",
        "outputId": "de0c0df1-840f-452c-b53a-9480e8792e31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "      \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    eval_macro_f1_score, eval_micro_f1_score = 0, 0 # new\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        tmp_eval_macro_f1_score = flat_macro_f1_score(logits, label_ids)\n",
        "        tmp_eval_micro_f1_score = flat_micro_f1_score(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        eval_macro_f1_score += tmp_eval_macro_f1_score\n",
        "        eval_micro_f1_score += tmp_eval_micro_f1_score\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Macro F1: {0:.2f}\".format(eval_macro_f1_score/nb_eval_steps))\n",
        "    print(\"  Micro F1: {0:.2f}\".format(eval_micro_f1_score/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 2.88\n",
            "  Training epoch took: 0:00:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.18\n",
            "  Macro F1: 0.11\n",
            "  Micro F1: 0.18\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 2.58\n",
            "  Training epoch took: 0:00:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "  Macro F1: 0.15\n",
            "  Micro F1: 0.25\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 2.26\n",
            "  Training epoch took: 0:00:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.36\n",
            "  Macro F1: 0.25\n",
            "  Micro F1: 0.36\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.82\n",
            "  Training epoch took: 0:00:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.43\n",
            "  Macro F1: 0.30\n",
            "  Micro F1: 0.43\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.38\n",
            "  Training epoch took: 0:00:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.51\n",
            "  Macro F1: 0.40\n",
            "  Micro F1: 0.51\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 0:00:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  Macro F1: 0.48\n",
            "  Micro F1: 0.57\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.79\n",
            "  Training epoch took: 0:00:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.61\n",
            "  Macro F1: 0.51\n",
            "  Micro F1: 0.61\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.63\n",
            "  Training epoch took: 0:00:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.64\n",
            "  Macro F1: 0.53\n",
            "  Micro F1: 0.64\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.51\n",
            "  Training epoch took: 0:00:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.64\n",
            "  Macro F1: 0.53\n",
            "  Micro F1: 0.64\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.48\n",
            "  Training epoch took: 0:00:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.64\n",
            "  Macro F1: 0.52\n",
            "  Micro F1: 0.64\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLKr8oeQcQ2-",
        "colab_type": "code",
        "outputId": "2aa01229-ebfc-4081-b003-aa69dd733f73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuUAAAGaCAYAAACopj13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERERkWD/D08qAWGhv8ZcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKUMkkdPyODc",
        "colab_type": "code",
        "outputId": "6c88f09a-2114-4bf1-c9b1-a7ecb5f92525",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-30715214-7906-4be2-8e4c-804a33eb189b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-30715214-7906-4be2-8e4c-804a33eb189b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving pool_comments.csv to pool_comments.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFGoyq7V59RH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"pool_comments.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRXFxkbZ6Nst",
        "colab_type": "code",
        "outputId": "b02785c3-3654-4a25-b214-cf2154715b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "df.head()\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "df.comment.values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 4,826\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Absolutely fabulous in both areas. Food and service is top rate. We actually compare outside restaurants and their service to ours here at Highland Springs. Always H.S. is best and our high school servers beautifully trained.',\n",
              "       'At the cafe, could we have an old-fashioned meatloaf on occasion?',\n",
              "       'Awesome! Amazing!', ...,\n",
              "       \"Would like to see a more appropriate space given to the Treasure Chest. They do a great job. But it's hard to work in cramped quarters which are also separated.\",\n",
              "       'You can see from my answer that I am a happy camper. The people and residents are good; the staff is excellent and all the workers are pleasant and helpful.',\n",
              "       'Your pumpkin pie is delicious. Do you think we might have 2 or maybe even three squirts of whipped cream?'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwGMhsXp6aSq",
        "colab_type": "code",
        "outputId": "fab95581-afaa-4987-8423-e218ab365651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "texts = df.comment.values\n",
        "# labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for text in texts:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_text = tokenizer.encode(\n",
        "                        text,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_text)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "# prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks) #, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 4,826\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYMK83kS_wPH",
        "colab_type": "code",
        "outputId": "dab4bfb7-ee74-4f4e-9d9d-7bb10e5170c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions = [ ]\n",
        "# true_labels = []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  # label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  # true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 4,826 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJIV5gMYBSIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions_logit = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions_class = np.argmax(flat_predictions_logit, axis=1).flatten()\n",
        "\n",
        "flat_predictions_class\n",
        "np.save('predictions', flat_predictions_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFEwE2UXU_Z4",
        "colab_type": "code",
        "outputId": "e2e9accb-7ae0-460c-f5e1-bb2cbd00949c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(flat_predictions_class)\n",
        "texts[2243]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Do it myself.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTCJs3YBVS_g",
        "colab_type": "code",
        "outputId": "db018e94-e8bb-4566-a263-c8c306263f29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "le.inverse_transform([flat_predictions_class[2243]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['other'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5n-SklwBiiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Determine the least certain predictions via their class probabilities\n",
        "\n",
        "flat_predictions_probs = np.exp(flat_predictions_logit) / (1 + np.exp(flat_predictions_logit))\n",
        "predictions_uncertainty = 1 - np.max(flat_predictions_probs, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OanhzPzeBkBv",
        "colab_type": "code",
        "outputId": "c0ac7241-e983-4fb3-eddf-f450e17cbe48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.max(predictions_uncertainty)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.26645738"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42a5PwxJBzlW",
        "colab_type": "code",
        "outputId": "9f79e906-c252-4cd2-b490-f6bf31c749f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.argmax(predictions_uncertainty)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1222"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19mU3GqrC5bT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "set_to_label = np.argsort(-predictions_uncertainty)[:100] # indices of maximum uncertainty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZlfWG12HgIR",
        "colab_type": "code",
        "outputId": "d6a5709b-0fee-4971-a893-e344af58c723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "texts[set_to_label]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['I am still new. Some things are as expected, others are inconsistent, or sales office did not know or was misleading. Resident population is older and frailer than expected. I would recommend some of Brooksby Village to family and friends.',\n",
              "       'We need more resident input over future plans and development and not only corporate planning. The new Community Room in Willow Ridge should never have been located where it is. My walk from my apartment is almost 1/4 of a mile. I am a retired architect and could certainly provide my knowledge in future planning.',\n",
              "       'Hired our own that was recommended. Very Satisfied.',\n",
              "       'Sometimes the Care feels like a stepchild. We should have an ice cream cart instead of a makeshift arrangement. The installation of a microwave has been a big plus.',\n",
              "       \"We have a new Resident Handbook that is very clear in regard to rules and regulations. I am concerned that residents cannot count on the Handbook being enforced. I believe that residents should be protected by rules of conduct as laid out in the Handbook. We have: trees in flowerbeds, birdbaths, patio awning that does not match our awnings, lawn decorations beyond patio boundaries', etc. While I don't use transportation it seems to me if it is promised it should be delivered. I am concerned that Fountain View being full and our gaining residents there will not be a place for us to go. Then our Care for Life does not apply.\",\n",
              "       'Why was the decision made to begin building another neighborhood? Expanding Fountain View should have received higher priority.',\n",
              "       \"I feel like I'm on vacation!\",\n",
              "       'There should be more handicap places, I fight for a place with a guy that has 2 cars & 2 parked places he pays for and still the (expletive) occupies the handicap.',\n",
              "       \"I like the idea of solar electric from collections on roofs over all parked resident cars. It's a win-win, with long-term payoff.\",\n",
              "       'A shower faucet that includes flow control as well as temperature control would be welcome. We would save water too because I would select a lower flow if I had the option to do so.',\n",
              "       'Prefer to do my own',\n",
              "       'I arrived as a complete stranger to the area and to Brooksby. It was very difficult entering the dining room alone but most residents are very friendly. Once I conquered that hurdle, living here became enjoyable. ',\n",
              "       \"How about campus wide Wi-Fi internet. It's becoming more and more expensive to purchase it. Earlier prices were in the $20.00 per month range and now they're $40.00 from Comcast. Some people are paying as much as $180.00 per month for telephone and internet from Verizon. Most of us don't know what we're buying or what we need. Even help from Brooksby on negotiating better rates would be welcome.\",\n",
              "       \"The mailroom needs to be moved out of the main lobby. There is constant congestion in that area with hundreds of people coming to check their mail every day. Multiple visits are required since it takes the postal staff hours to place mail in the boxes. While residents are waiting for their mail they stand at the desk and chitchat with the very busy staff (who are excellent). It's difficult to transact business under these conditions. Potential residents who have been invited to the facility cannot find a seat, get to the desk, or get through the congestion. Not a good first impression. The waiting time for the Sunday buffet is at least 45 minutes. Many of the residents are using walkers or wheelchairs and have a very difficult time getting their food from the serving line. Perhaps additional staff should be available to bring food to those in need. There is a line for omelets, other breakfast foods, waffles, pastry ice cream and pie, salads, etc. Very difficult for people requiring assistance.\",\n",
              "       'We have been residents at E.T. for 8 1/2 years. We chose E.t. because when one of us is living at Bayou Vista (Renaissance Garden) the other can easily visit there. This is what marketing told us! This is not true if: It is too cold to go outside, it is too hot to go outside, if it is raining, if the ET spouse  has a walker, etc.. In such circumstances Bayou Vista might as well be somewhere else entirely. What do you have to say? An answer would be appropriate!  ',\n",
              "       'Distant apartments from central area with less walking. Thank you.',\n",
              "       \"Have residents shut lights when not in apartment. Don't run water needlessly.\",\n",
              "       'Is as good as can be expected. Being only three month here it is too early to give a fair evaluation. We are learning to adjust to our new environment.',\n",
              "       'Would really love a modest dog park.',\n",
              "       'Too expensive, only does simple surfaces work, watch the clock - to leave.',\n",
              "       'I prefer to do my own!',\n",
              "       'We are capable of taking care of our unit.',\n",
              "       'I wish Highland Springs could help retard the amount of U.S. Mail (junk) from being delivered to my mailbox. I am loaded every day.',\n",
              "       'I prefer to do my own.',\n",
              "       'I would like to have a car wash facility on the campus.',\n",
              "       'Pleasant - joyful. Pool constant noise must be fixed!',\n",
              "       'For anyone with a hearing loss the sound in the dining rooms is almost unbearable. Attention to a better acoustical ceiling or whatever it takes to improve this flaw should and could be addressed. Many complain to no avail.',\n",
              "       'It surely not at all the same as when I lived at home. They are always organized; I always had responsibilities, that day not mean that I have no one here. I do lever some here also, hard to explain the difference. Responsibilities exist either place, they keep us going. ',\n",
              "       'No take out available at Harvest do not see only not. Other buildings have.',\n",
              "       'To have all residents wear their name tags.',\n",
              "       'Implement noise abatement in Brooksby restaurants. Upgrade the Broadband.',\n",
              "       'Implement noise abatement in Brooksby restaurants. Upgrade the broadband backbone at Brooksby Village.',\n",
              "       'Package delivered but I got no notice. Had to ask front desk staff. Could use more sidewalk lighting. Stairwells rarely cleaned. After 9 years we got some plantings. Transportation service has improved. Definitely have confidence that security and/or emergency services would respond quickly and competently. A friend who is considering locating here said she felt guest parking NOT adequate. Often Highland Springs vehicle is parked in guest parking space.',\n",
              "       \"More orientation for new residents. Many of us during the hustle and bustle of moving do not have (or take) the time to read all the printed material foisted upon us. I have learned that I know things that some other new residents don't know, and some others know things that I don't know. I suggest monthly follow up meetings for each new resident group for the first several months. (also I was given incorrect info. by depts. that didn't know overall policy) All who enter in the same month are combined with another “group.”  \",\n",
              "       'Service is sometimes slow.',\n",
              "       'Front desk staff is the best. Transportation service is limited. Employees disappear, some before you know their names, i.e. “supervisors” - vacancies extended. Congenial folks ask “to leave.” Beautiful “grounds” but the “company” changes so often. Why?',\n",
              "       'This is the second time I have written, the tables in the Cider House pub are not what they should be, scratches are there and I am not happy bringing friends for lunch there, the chairs need to be changed so that what is spilled on them can be wiped off in other words leather or plastic.',\n",
              "       'It would be an improvement if the shower heads where you could sit for part of your shower.',\n",
              "       'Feel like I am on vacation here.',\n",
              "       'Have been bothered by fruit flies for quite some time. Exterminator has tried to get rid of them, but so far unsuccessful.',\n",
              "       \"Management could have some dialogue with the Post Office handling mail delivery. H. Springs is large and more buildings are being constructed - delivery should begin in the mornings, rather than 1 or 2 in the afternoon because completion occurs after 5 p.m. or 6 p.m. This especially occurs on Monday or Tuesday. As we get larger, not enough people are being hired as housekeepers, maintenance people, or transportation people. It's a shame because people can only do their best, but overwork doesn't allow them time to do their best.\",\n",
              "       'I was disappointed with the result the few times I used them. They missed a lot of areas. I use an outside service and get excellent results.',\n",
              "       \"It seems strange that Brooksby can't afford to always have a few papers in the lobbies - Globe, NY Times, local paper e.g. Salem News.\",\n",
              "       'Able to maintain ourselves.',\n",
              "       'I can still do it, easy unit to maintain, is another way to stay active.',\n",
              "       'There is a “treasure” at Highland Springs and it is the Market and the woman who runs it named Nancy Herbert. She does all the buying including two trips a year to the Dallas Market. She buys cute tops, purses, jewelry, and clever things for the house or apartment. She shops each week locally for everyday necessities. She buys frozen foods, cold meat, and cheese, milk, and specialty drinks. Each item has to be marked before going on the shelf. She has counters of candy, canned food, and soups - one area of the store for health needs, bandages, and containers for stomach problems, eye problems, toothpaste, and the other side of the store for needed things for your study or office and for laundry products. Each item has to be marked. First thing in the morning she acts as a “fast food” call. She makes breakfast. People not only come to buy or eat but also residents love to talk to Nancy. She is the family therapist for Highland Springs. Also every three months she takes inventory of all the store items - not many people could or would not be capable to do all this but also to be willing with a smile on your face. Nancy Herbert is a true “treasure.”',\n",
              "       'I like our high quality level of handling Administering the above tasks.',\n",
              "       \"Requested a light bulb change on Thursday and was booked for the following Tuesday! I had the light bulb ready! Need quick fix option for small jobs, you can't queue all types of jobs effectively.\",\n",
              "       'I use someone from Danvers who has been helping me for 20 years.',\n",
              "       \"Brooksby in general is a great place to live. I feel secure, well taken care of, relaxed, and happy mostly. However, since I am among the younger people living here and actually worked full time for quite some time after moving in, I see some areas which would make my life here more pleasurable. The most important to me is that very few activities, if any, are designed for active people under 80. There are walks from the fitness center, but there are no hiking or skiing trips. Dance music on dance night past the 1940's!! Food, or a bar, open after 7:00 p.m. Also, I find the artwork, upholstery, carpeting, and draperies, in public spaces to be depressing and looking as if they come out of a 1950's sensibility. There may be push back from some residents, but others would jump for joy for some modern art on the walls and more lively color for when visitors enter our public spaces. \",\n",
              "       'BBV has deteriorated in the 10 years I have lived here, primarily in the quality of food served in the various restaurants and the admission standards for new residents such as health, both physical and mental as well as financial. There are too many wheel chairs and walkers around. ',\n",
              "       'Great landscaping. Always there when needed. Terrific help.',\n",
              "       'Our guests’ luggage racks in guest apartments. More frequent window cleaning. Cost effective discount to upgrade apartments after 15-10 years - kitchens, rugs especially age.',\n",
              "       'Living here meet my NEEDS at my AGE. I feel safe, comfortable and enjoy the activities. ',\n",
              "       'I use an independent lady.',\n",
              "       'I love Brooksby. Brooksby is a wonderful place to live. The grounds are pristine and kept beautiful with flowering bushes, plants, and trees except for the surroundings around Devonshire Way. That should be as beautiful as the rest of the campus. We also need much better lighting at the parking lot. ',\n",
              "       'A permanent installation of a bocce courts would be of great value and relatively inexpensive to install and maintain.',\n",
              "       \"I've been with Erickson for 20 years - 11.5 @ Charlestown & 8.5 here @ Eagle's Trace, so far you can see I am very satisfied with Erickson & Eagle's Trace.\",\n",
              "       'Put digital pianos in catering and chapel. I am too small to function on the grand pianos. Pay more attention to needs of smaller people everywhere.',\n",
              "       'The Beauty Salon needs upgrading. It is ten years old and has had much use in that time.',\n",
              "       'I have heard of robberies. Too many master keys are allowed.',\n",
              "       'I have a sitter who is furnished by a private company.',\n",
              "       'I am the maid - good exercise.',\n",
              "       'I wish there was some way to uniform residents if a member is seriously ill and hospitalized. Three of my friends were really sick and I did not know about it. Privacy is important but ill people need a sign of caring (a not) from their friends and acquaintances.',\n",
              "       'We have excellent outside help.',\n",
              "       'My grandson calls this a \"resort!\"', 'I like to do my own.',\n",
              "       'We need to acknowledge First Responders on every anniversary of 9/11.',\n",
              "       'Elevator (single CT) is out of order too often.',\n",
              "       'Fountain View is at 100% capacity now, and with the addition of two new buildings, there appears to be no concrete plan on how to handle this.',\n",
              "       'Prefer to do our own.', 'Comfortable.', 'Still do my own.',\n",
              "       \"I don't have any friends who could afford Brooksby. My family does not live in US.\",\n",
              "       \"Some stuff members go beyond to help. There are some who don't and never respond or help in timely manner.\",\n",
              "       'Like to do my own- daughter helps.',\n",
              "       'I am still employed five days a week.',\n",
              "       \"I feel I can make a 100 here at Eagle's Trace!\",\n",
              "       'Instead of having to go down to lobby at town center to mail a letter, please install \"mail chutes, so we can mail letters from the upper floors. It would be much more convenient and appreciated by all.',\n",
              "       'Some guests don\\'t put out trash when suggested. Some put any kind out \"7\" days a week.',\n",
              "       \"The beauty shop needs a face lift after 10 years. Don't you think??\",\n",
              "       'The after-hours crew is smaller nit they respond reasonably well as needed. I am informed about activities but of course one has to keep their eyes and ears open. Smooth as a whistle.',\n",
              "       'Oh, do I wish it could be worked out to have a small dog park (fenced). It would add a rich aspect to our life here. There must be a modest space for this on the BBV property. Thank you for considering this. Would be willing to participate in planning, etc.',\n",
              "       'I need grass around my patio! Nothing but black clay here around my patio!!',\n",
              "       'I moved from New York State and I like it here.',\n",
              "       'Should have better set up for films. They charge too much for fare on trips.',\n",
              "       'I have not been in residence here long enough and have not been exposed to the involvement of residents opportunity for involvement in operations of variable committees.',\n",
              "       'Landscaping and exterior grounds color could be brighter, particularly at entrance. Planning seems to not recognizing that Texans drive cars longer and more TX own cars than plan has provided space for and from glimpse of future campus plans this will only get worse. We are not easterners in this regard.',\n",
              "       \"Moving to Brooksby eleven and a half years ago was the best thing my husband and I ever did. The support and services available here are phenomenal. My husband's death, three and a half years ago, made me appreciate Brooksby even more. The support I received from security, EMT's, staff, medical center, home support, and visiting nurses, I would never have had any of this if I were not here at Brooksby.\",\n",
              "       \"I am a new resident of two months- don't feel qualified to make a strong opinion. So far it has been a pleasant experience.\",\n",
              "       'It is said that assistance is very slow in responding when requested.',\n",
              "       'Many of the perks promised when I moved in have been discontinued.',\n",
              "       'My maid has been with me since before I came here.',\n",
              "       'Caregivers help some. I do what I can. My cousin helps me.',\n",
              "       'Too many residents use the handicap spaces day and night. Not leaving those spaces open for our families or guest to use. Not fair to the rest of us!',\n",
              "       'General Services has improved a great deal in last few years.',\n",
              "       \"I've always had responsive answers to my questions. if I had a maintenance problem it was solved in a timely manner. The Housekeeping Staff are on time thorough, caring and do a great job. When problems occasionally happen community wide like water or electrical interruptions we are informed promptly and efficiently. Things are well explained through a variety of sources: robot call, cubby stuffer, Eagle Vista News.\",\n",
              "       'I would like to know more about the single ladies in my age group, which is much younger than people who have been here for 5 to 15 years. I am 74 years old.',\n",
              "       'Help very slow in answering bell in continuing care, not enough helpers on  weekends. Also, in dining room.',\n",
              "       'Proviso re Ah or LTC - can “promises” be trusted? Are they reliable? A nagging concern is what is “promised” now will not be fulfilled in the future. A current “promise” is the comparable assisted living will is provided should continuing care space not be available. A previous “promise” was a list for available garage space without a deposit. That “promise” was changed to a “sliding” payment rate for residents on list prior to a change in procedure date.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOK-JpXIJzGP",
        "colab_type": "code",
        "outputId": "3051af29-4880-4081-d002-90af44505869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>segment</th>\n",
              "      <th>comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Absolutely fabulous in both areas. Food and se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>At the cafe, could we have an old-fashioned me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Awesome! Amazing!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Being served by student helpers is a pleasant ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Buffet items are rarely hot. Other than that d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4821</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>Why no questions re: Marketing? Staff seems un...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4822</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>With the full assurance that friends &amp; neighbo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4823</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>Would like to see a more appropriate space giv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4824</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>You can see from my answer that I am a happy c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4825</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>Your pumpkin pie is delicious. Do you think we...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4826 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     segment                                            comment\n",
              "0     IL-HSD  Absolutely fabulous in both areas. Food and se...\n",
              "1     IL-HSD  At the cafe, could we have an old-fashioned me...\n",
              "2     IL-HSD                                  Awesome! Amazing!\n",
              "3     IL-HSD  Being served by student helpers is a pleasant ...\n",
              "4     IL-HSD  Buffet items are rarely hot. Other than that d...\n",
              "...      ...                                                ...\n",
              "4821  IL-ETH  Why no questions re: Marketing? Staff seems un...\n",
              "4822  IL-ETH  With the full assurance that friends & neighbo...\n",
              "4823  IL-ETH  Would like to see a more appropriate space giv...\n",
              "4824  IL-ETH  You can see from my answer that I am a happy c...\n",
              "4825  IL-ETH  Your pumpkin pie is delicious. Do you think we...\n",
              "\n",
              "[4826 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0_c0utdSfjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts_to_label = df.iloc[set_to_label]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXqrtAArVPdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save texts to label to file\n",
        "texts_to_label.to_csv(\"./texts_to_label.csv\", encoding='utf-8', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwwPN2NsS1eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_drop = df.drop(set_to_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt_k0rfHTbYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop sets to label from the training pool\n",
        "df_drop.to_csv(\"./pool_comments_update.csv\", encoding='utf-8', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXKbDQfIvfRD",
        "colab_type": "code",
        "outputId": "18e570cf-5f48-46e8-839f-f81c235b844c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "texts_to_label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>segment</th>\n",
              "      <th>comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1222</th>\n",
              "      <td>IL-BBV</td>\n",
              "      <td>I am still new. Some things are as expected, o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1148</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>We need more resident input over future plans ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2324</th>\n",
              "      <td>IL-BBV</td>\n",
              "      <td>Hired our own that was recommended. Very Satis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4007</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>Sometimes the Care feels like a stepchild. We ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1140</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>We have a new Resident Handbook that is very c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4083</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>General Services has improved a great deal in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4109</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>I've always had responsive answers to my quest...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3553</th>\n",
              "      <td>IL-BBV</td>\n",
              "      <td>I would like to know more about the single lad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4668</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>Help very slow in answering bell in continuing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>601</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Proviso re Ah or LTC - can “promises” be trust...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     segment                                            comment\n",
              "1222  IL-BBV  I am still new. Some things are as expected, o...\n",
              "1148  IL-HSD  We need more resident input over future plans ...\n",
              "2324  IL-BBV  Hired our own that was recommended. Very Satis...\n",
              "4007  IL-ETH  Sometimes the Care feels like a stepchild. We ...\n",
              "1140  IL-HSD  We have a new Resident Handbook that is very c...\n",
              "...      ...                                                ...\n",
              "4083  IL-ETH  General Services has improved a great deal in ...\n",
              "4109  IL-ETH  I've always had responsive answers to my quest...\n",
              "3553  IL-BBV  I would like to know more about the single lad...\n",
              "4668  IL-ETH  Help very slow in answering bell in continuing...\n",
              "601   IL-HSD  Proviso re Ah or LTC - can “promises” be trust...\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 253
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlLrB09swSp9",
        "colab_type": "code",
        "outputId": "5e117c54-1d14-4fc5-bada-425d2edb56fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df_drop"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>segment</th>\n",
              "      <th>comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Absolutely fabulous in both areas. Food and se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>At the cafe, could we have an old-fashioned me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Awesome! Amazing!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Being served by student helpers is a pleasant ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Buffet items are rarely hot. Other than that d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4821</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>Why no questions re: Marketing? Staff seems un...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4822</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>With the full assurance that friends &amp; neighbo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4823</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>Would like to see a more appropriate space giv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4824</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>You can see from my answer that I am a happy c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4825</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>Your pumpkin pie is delicious. Do you think we...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4726 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     segment                                            comment\n",
              "0     IL-HSD  Absolutely fabulous in both areas. Food and se...\n",
              "1     IL-HSD  At the cafe, could we have an old-fashioned me...\n",
              "2     IL-HSD                                  Awesome! Amazing!\n",
              "3     IL-HSD  Being served by student helpers is a pleasant ...\n",
              "4     IL-HSD  Buffet items are rarely hot. Other than that d...\n",
              "...      ...                                                ...\n",
              "4821  IL-ETH  Why no questions re: Marketing? Staff seems un...\n",
              "4822  IL-ETH  With the full assurance that friends & neighbo...\n",
              "4823  IL-ETH  Would like to see a more appropriate space giv...\n",
              "4824  IL-ETH  You can see from my answer that I am a happy c...\n",
              "4825  IL-ETH  Your pumpkin pie is delicious. Do you think we...\n",
              "\n",
              "[4726 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gymHLjWOwWPS",
        "colab_type": "code",
        "outputId": "8807882c-26ea-47e1-a56a-823af4f6038c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>segment</th>\n",
              "      <th>comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Absolutely fabulous in both areas. Food and se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>At the cafe, could we have an old-fashioned me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Awesome! Amazing!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Being served by student helpers is a pleasant ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Buffet items are rarely hot. Other than that d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4821</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>Why no questions re: Marketing? Staff seems un...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4822</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>With the full assurance that friends &amp; neighbo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4823</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>Would like to see a more appropriate space giv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4824</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>You can see from my answer that I am a happy c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4825</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>Your pumpkin pie is delicious. Do you think we...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4826 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     segment                                            comment\n",
              "0     IL-HSD  Absolutely fabulous in both areas. Food and se...\n",
              "1     IL-HSD  At the cafe, could we have an old-fashioned me...\n",
              "2     IL-HSD                                  Awesome! Amazing!\n",
              "3     IL-HSD  Being served by student helpers is a pleasant ...\n",
              "4     IL-HSD  Buffet items are rarely hot. Other than that d...\n",
              "...      ...                                                ...\n",
              "4821  IL-ETH  Why no questions re: Marketing? Staff seems un...\n",
              "4822  IL-ETH  With the full assurance that friends & neighbo...\n",
              "4823  IL-ETH  Would like to see a more appropriate space giv...\n",
              "4824  IL-ETH  You can see from my answer that I am a happy c...\n",
              "4825  IL-ETH  Your pumpkin pie is delicious. Do you think we...\n",
              "\n",
              "[4826 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgUN2sS8W-P7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_preds = pd.DataFrame(flat_predictions_class)\n",
        "df['preds']= df_preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO0UNtpiXLHX",
        "colab_type": "code",
        "outputId": "3e040e14-30c1-42c1-e750-1b54d79bd951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>segment</th>\n",
              "      <th>comment</th>\n",
              "      <th>preds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Absolutely fabulous in both areas. Food and se...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>At the cafe, could we have an old-fashioned me...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Awesome! Amazing!</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Being served by student helpers is a pleasant ...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IL-HSD</td>\n",
              "      <td>Buffet items are rarely hot. Other than that d...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4821</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>Why no questions re: Marketing? Staff seems un...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4822</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>With the full assurance that friends &amp; neighbo...</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4823</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>Would like to see a more appropriate space giv...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4824</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>You can see from my answer that I am a happy c...</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4825</th>\n",
              "      <td>IL-ETH</td>\n",
              "      <td>Your pumpkin pie is delicious. Do you think we...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4826 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     segment                                            comment  preds\n",
              "0     IL-HSD  Absolutely fabulous in both areas. Food and se...      7\n",
              "1     IL-HSD  At the cafe, could we have an old-fashioned me...      7\n",
              "2     IL-HSD                                  Awesome! Amazing!     14\n",
              "3     IL-HSD  Being served by student helpers is a pleasant ...      7\n",
              "4     IL-HSD  Buffet items are rarely hot. Other than that d...      7\n",
              "...      ...                                                ...    ...\n",
              "4821  IL-ETH  Why no questions re: Marketing? Staff seems un...     11\n",
              "4822  IL-ETH  With the full assurance that friends & neighbo...     17\n",
              "4823  IL-ETH  Would like to see a more appropriate space giv...      5\n",
              "4824  IL-ETH  You can see from my answer that I am a happy c...     17\n",
              "4825  IL-ETH  Your pumpkin pie is delicious. Do you think we...      6\n",
              "\n",
              "[4826 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTbjfxesYB7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(\"./labeled_pool_comments.csv\", encoding='utf-8', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rDqLurxbPRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}